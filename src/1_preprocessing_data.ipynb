{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "fac7d5ac",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/oscarjimenezbou/Documents/TFG_ADE/code/TFG/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "sys.path.append(\"..\")\n",
        "from utilities import *\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "raw_data = pd.read_excel(\"../data/Datos_Market_copy.xlsx\")\n",
        "sa = SalesAnalysis(raw_data)\n",
        "data = sa.data \n",
        "\n",
        "# SERIES ID\n",
        "data[\"date\"] = pd.to_datetime(data[\"date\"])\n",
        "\n",
        "data['series_id'] = (\n",
        "    data[\"brand\"].astype(str) + \"_\" +\n",
        "    data[\"supermarket\"].astype(str) + \"_\" +\n",
        "    data[\"variant\"].astype(str) + \"_\" +\n",
        "    data[\"pack.size\"].astype(str)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a9651ff",
      "metadata": {},
      "source": [
        "LIMPIEZA DE DATOS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "6b1ca156",
      "metadata": {},
      "outputs": [],
      "source": [
        "def clean_data(data: pd.DataFrame) -> pd.DataFrame:\n",
        "\n",
        "    data['volume.sales'] = np.log1p(data['volume.sales'])\n",
        "    data = data[data['volume.sales'] >= 6] \n",
        "    data['volume.sales'] = np.expm1(data['volume.sales'])\n",
        "\n",
        "    df = data.copy()\n",
        "\n",
        "    df = df.sort_values([\"series_id\", \"date\"]).reset_index(drop=True) \n",
        "    g = df.groupby(\"series_id\")[\"volume.sales\"]\n",
        "\n",
        "    profile = pd.DataFrame({\n",
        "        \"n\": g.size(),\n",
        "        \"zeros\": g.apply(lambda s: (s <= 0).sum()),\n",
        "        \"pct_zeros\": g.apply(lambda s: (s <= 0).mean()),\n",
        "        \"mean\": g.mean(),\n",
        "        \"median\": g.median(),\n",
        "        \"p10\": g.quantile(0.10),\n",
        "        \"p90\": g.quantile(0.90),\n",
        "        \"std\": g.std()\n",
        "    }).reset_index()\n",
        "\n",
        "    profile[\"cv\"] = profile[\"std\"] / profile[\"mean\"].replace(0, np.nan)  # coef. variaci√≥n\n",
        "    profile = profile.sort_values(\"pct_zeros\", ascending=False)\n",
        "\n",
        "    '''\n",
        "    CREAR BUCKETS DE ESCALA\n",
        "    short_hist = True ‚Üí no merece la pena ARIMAX serio (muy inestable). Mejor baseline.\n",
        "    micro/low ‚Üí suelen comportarse distinto (ruido relativo enorme). No las ‚Äúelimines‚Äù: tr√°talas con otra estrategia.\n",
        "    '''\n",
        "\n",
        "    bins = [-np.inf, 100, 1000, 10000, 50000, np.inf]\n",
        "    labels = [\"micro(<100)\", \"low(100-1k)\", \"mid(1k-10k)\", \"high(10k-50k)\", \"very_high(>50k)\"]\n",
        "\n",
        "    profile[\"scale_bucket\"] = pd.cut(profile[\"mean\"], bins=bins, labels=labels)\n",
        "    profile[\"short_hist\"] = profile[\"n\"] < 24  # <2 a√±os mensual\n",
        "\n",
        "    profile[[\"series_id\",\"n\",\"mean\",\"scale_bucket\",\"short_hist\"]].sort_values([\"short_hist\",\"mean\"])\n",
        "\n",
        "    arimax_ids = profile.loc[~profile[\"short_hist\"] & profile[\"scale_bucket\"].isin([\"mid(1k-10k)\",\"high(10k-50k)\",\"very_high(>50k)\"]), \"series_id\"]\n",
        "    micro_low_ids = profile.loc[profile[\"scale_bucket\"].isin([\"micro(<100)\",\"low(100-1k)\"]), \"series_id\"]\n",
        "    short_ids = profile.loc[profile[\"short_hist\"], \"series_id\"]\n",
        "\n",
        "    len(arimax_ids), len(micro_low_ids), len(short_ids)\n",
        "    print(len(arimax_ids), len(micro_low_ids), len(short_ids))\n",
        "\n",
        "    data_clean = df[df['series_id'].isin(arimax_ids)].copy()\n",
        "\n",
        "    data_clean['volume.sales'] = np.log1p(data_clean['volume.sales'])\n",
        "    data_clean = data_clean[data_clean['volume.sales'] >= 6] \n",
        "    data_clean['volume.sales'] = np.expm1(data_clean['volume.sales']) \n",
        "\n",
        "    return data_clean\n",
        "\n",
        "# data= clean_data(data_clean)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "31b97eb7",
      "metadata": {},
      "outputs": [],
      "source": [
        "def distributions_variables(data: pd.DataFrame):\n",
        "\n",
        "    # Estad√≠sticas descriptivas\n",
        "    print(\"=\" * 60)\n",
        "    print(\"üìä ESTAD√çSTICAS DESCRIPTIVAS - VARIABLES NUM√âRICAS\")\n",
        "    print(\"=\" * 60)\n",
        "    print(data[['volume.sales', 'price']].describe())\n",
        "\n",
        "    # Crear figura con subplots\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "    fig.suptitle('Distribuci√≥n de Variables', fontsize=14, fontweight='bold')\n",
        "\n",
        "    # 1. Histograma de volume.sales en escala logar√≠tmica (log de la variable)\n",
        "    log_volume = np.log1p(data['volume.sales'])  # log(1 + x) para evitar log(0)\n",
        "    # log_volume = data['volume.sales']\n",
        "    ax1 = axes[0, 0]\n",
        "    ax1.hist(log_volume, bins=50, edgecolor='black', alpha=0.7, color='steelblue')\n",
        "    ax1.set_xlabel('log(1 + Volume Sales)')\n",
        "    ax1.set_ylabel('Frecuencia')\n",
        "    ax1.set_title('Distribuci√≥n de Volume.Sales (log)')\n",
        "    ax1.axvline(log_volume.mean(), color='red', linestyle='--', label=f'Media (log): {log_volume.mean():.2f}')\n",
        "    ax1.axvline(log_volume.median(), color='green', linestyle='--', label=f'Mediana (log): {log_volume.median():.2f}')\n",
        "    ax1.legend()\n",
        "\n",
        "    # 2. Histograma de price\n",
        "    ax2 = axes[0, 1]\n",
        "    ax2.hist(data['price'], bins=50, edgecolor='black', alpha=0.7, color='coral')\n",
        "    ax2.set_xlabel('Price')\n",
        "    ax2.set_ylabel('Frecuencia')\n",
        "    ax2.set_title('Distribuci√≥n de Price')\n",
        "    ax2.axvline(data['price'].mean(), color='red', linestyle='--', label=f'Media: {data[\"price\"].mean():.2f}')\n",
        "    ax2.axvline(data['price'].median(), color='green', linestyle='--', label=f'Mediana: {data[\"price\"].median():.2f}')\n",
        "    ax2.legend()\n",
        "\n",
        "    # 3. Boxplot de volume.sales\n",
        "    ax3 = axes[0, 2]\n",
        "    ax3.boxplot(data['volume.sales'].dropna(), vert=True)\n",
        "    ax3.set_ylabel('Volume Sales')\n",
        "    ax3.set_title('Boxplot Volume.Sales')\n",
        "\n",
        "    # 4. Boxplot de price\n",
        "    ax4 = axes[1, 0]\n",
        "    ax4.boxplot(data['price'].dropna(), vert=True)\n",
        "    ax4.set_ylabel('Price')\n",
        "    ax4.set_title('Boxplot Price')\n",
        "\n",
        "    # 5. Volume.sales por brand\n",
        "    ax5 = axes[1, 1]\n",
        "    data.boxplot(column='volume.sales', by='brand', ax=ax5)\n",
        "    ax5.set_xlabel('Brand')\n",
        "    ax5.set_ylabel('Volume Sales')\n",
        "    ax5.set_title('Volume.Sales por Brand')\n",
        "    plt.suptitle('')  # Eliminar t√≠tulo autom√°tico\n",
        "\n",
        "    # 6. Price por brand\n",
        "    ax6 = axes[1, 2]\n",
        "    data.boxplot(column='price', by='brand', ax=ax6)\n",
        "    ax6.set_xlabel('Brand')\n",
        "    ax6.set_ylabel('Price')\n",
        "    ax6.set_title('Price por Brand')\n",
        "    plt.suptitle('')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # ============================================================\n",
        "    # Distribuci√≥n por categor√≠as\n",
        "    # ============================================================\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"üìä DISTRIBUCI√ìN POR CATEGOR√çAS\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    fig2, axes2 = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "    # Conteo por brand\n",
        "    ax_brand = axes2[0, 0]\n",
        "    brand_counts = data['brand'].value_counts()\n",
        "    brand_counts.plot(kind='bar', ax=ax_brand, color='steelblue', edgecolor='black')\n",
        "    ax_brand.set_xlabel('Brand')\n",
        "    ax_brand.set_ylabel('N√∫mero de registros')\n",
        "    ax_brand.set_title('Distribuci√≥n por Brand')\n",
        "    ax_brand.tick_params(axis='x', rotation=45)\n",
        "\n",
        "    # Conteo por supermarket\n",
        "    ax_super = axes2[0, 1]\n",
        "    super_counts = data['supermarket'].value_counts()\n",
        "    super_counts.plot(kind='bar', ax=ax_super, color='coral', edgecolor='black')\n",
        "    ax_super.set_xlabel('Supermarket')\n",
        "    ax_super.set_ylabel('N√∫mero de registros')\n",
        "    ax_super.set_title('Distribuci√≥n por Supermarket')\n",
        "    ax_super.tick_params(axis='x', rotation=45)\n",
        "\n",
        "    # Conteo por variant\n",
        "    ax_var = axes2[1, 0]\n",
        "    var_counts = data['variant'].value_counts()\n",
        "    var_counts.plot(kind='bar', ax=ax_var, color='seagreen', edgecolor='black')\n",
        "    ax_var.set_xlabel('Variant')\n",
        "    ax_var.set_ylabel('N√∫mero de registros')\n",
        "    ax_var.set_title('Distribuci√≥n por Variant')\n",
        "    ax_var.tick_params(axis='x', rotation=45)\n",
        "\n",
        "    # Conteo por pack.size\n",
        "    ax_pack = axes2[1, 1]\n",
        "    pack_counts = data['pack.size'].value_counts()\n",
        "    pack_counts.plot(kind='bar', ax=ax_pack, color='mediumpurple', edgecolor='black')\n",
        "    ax_pack.set_xlabel('Pack Size')\n",
        "    ax_pack.set_ylabel('N√∫mero de registros')\n",
        "    ax_pack.set_title('Distribuci√≥n por Pack.Size')\n",
        "    ax_pack.tick_params(axis='x', rotation=45)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # ============================================================\n",
        "    # Correlaci√≥n entre variables num√©ricas\n",
        "    # ============================================================\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"üìä CORRELACI√ìN: VOLUME.SALES vs PRICE\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    correlation = data['volume.sales'].corr(data['price'])\n",
        "    print(f\"Correlaci√≥n de Pearson: {correlation:.4f}\")\n",
        "\n",
        "    fig3, ax = plt.subplots(figsize=(8, 6))\n",
        "    ax.scatter(data['price'], data['volume.sales'], alpha=0.5, edgecolors='black', linewidth=0.5)\n",
        "    ax.set_xlabel('Price')\n",
        "    ax.set_ylabel('Volume Sales')\n",
        "    ax.set_title(f'Relaci√≥n Price vs Volume.Sales (r = {correlation:.3f})')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2a0fd04",
      "metadata": {},
      "source": [
        "TRANSFORMACI√ìN BOX-COX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "1fb69617",
      "metadata": {},
      "outputs": [],
      "source": [
        "def boxcox_transform(y, warning=False):\n",
        "    # ========== TRANSFORMACI√ìN BOX-COX DE LA VARIABLE OBJETIVO ==========\n",
        "    # Verificar que volume.sales sea positiva para aplicar Box-Cox\n",
        "\n",
        "    # y = data['volume.sales'].copy() \n",
        "\n",
        "    if warning:\n",
        "        print(\"üìä Verificaci√≥n de datos para transformaci√≥n Box-Cox:\")\n",
        "        print(f\"   - Longitud de datos: {len(y)}\")\n",
        "        print(f\"   - Valor m√≠nimo: {y.min():.6f}\")\n",
        "        print(f\"   - Valor m√°ximo: {y.max():.6f}\")\n",
        "        print(f\"   - Valores <= 0: {(y <= 0).sum()}\")\n",
        "        print(f\"   - Media: {y.mean():.2f}\")\n",
        "\n",
        "    # Box-Cox requiere valores estrictamente positivos\n",
        "    if (y <= 0).any():\n",
        "        # Si hay ceros o negativos, necesitamos hacer un shift\n",
        "        min_positive = y[y > 0].min() if (y > 0).any() else 1.0\n",
        "        constant = max(1.0, min_positive)\n",
        "        y_shifted = y + constant\n",
        "        if warning:\n",
        "            print(f\"   ‚ö†Ô∏è  Se detectaron valores <= 0. Aplicando shift: y + {constant:.6f}\")\n",
        "    else:\n",
        "        y_shifted = y.copy()\n",
        "        constant = 0.0\n",
        "        if warning:\n",
        "            print(f\"   ‚úÖ Todos los valores son positivos. No se requiere shift\")\n",
        "\n",
        "    # Aplicar transformaci√≥n Box-Cox\n",
        "    # scipy.stats.boxcox encuentra el lambda √≥ptimo y aplica la transformaci√≥n\n",
        "    y_boxcox, lambda_optimal = stats.boxcox(y_shifted)\n",
        "\n",
        "    if warning:\n",
        "        print(f\"\\nüìà Estad√≠sticas despu√©s de la transformaci√≥n Box-Cox:\")\n",
        "        print(f\"   - Lambda √≥ptimo: {lambda_optimal:.6f}\")\n",
        "        print(f\"   - Media Box-Cox: {y_boxcox.mean():.6f}\")\n",
        "        print(f\"   - Desviaci√≥n est√°ndar Box-Cox: {y_boxcox.std():.6f}\")\n",
        "        print(f\"   - Constante de shift aplicada: {constant:.6f}\")\n",
        "\n",
        "    # Guardar informaci√≥n de la transformaci√≥n para reversi√≥n futura\n",
        "    boxcox_transformation_info = {\n",
        "        'type': 'boxcox',\n",
        "        'lambda': lambda_optimal,\n",
        "        'constant': constant\n",
        "    }\n",
        "\n",
        "    # Nota: Para revertir la transformaci√≥n Box-Cox:\n",
        "    # y_reversed = np.power(y_boxcox * lambda_optimal + 1, 1/lambda_optimal) - constant\n",
        "    # Si lambda_optimal == 0, entonces: y_reversed = np.exp(y_boxcox) - constant\n",
        "    return y_boxcox, boxcox_transformation_info\n",
        "    \n",
        "def inverse_boxcox(y_transformed, lambda_val: float, constant: float = 0.0):\n",
        "    \"\"\"\n",
        "    Inversa de la transformaci√≥n Box-Cox usada en `boxcox_transform`.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    y_transformed : array-like\n",
        "        Valores transformados\n",
        "    lambda_val : float\n",
        "        Lambda de Box-Cox\n",
        "    constant : float\n",
        "        Shift aplicado antes del Box-Cox (se resta al final)\n",
        "    \"\"\"\n",
        "    y_transformed = np.asarray(y_transformed)\n",
        "    if lambda_val == 0:\n",
        "        return np.exp(y_transformed) - constant\n",
        "    return np.power(y_transformed * lambda_val + 1, 1 / lambda_val) - constant"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5b65e95",
      "metadata": {},
      "source": [
        "ELIMINAR SERIES CONFLICTIVAS DEL MODELO\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "863efd3b",
      "metadata": {},
      "outputs": [],
      "source": [
        "if False:\n",
        "    def remove_conflictive_series(\n",
        "        df: pd.DataFrame,\n",
        "        brand: str = None,\n",
        "        supermarket: str = None,\n",
        "        variant: str = None,\n",
        "        pack_size: str = None,\n",
        "        verbose: bool = True\n",
        "    ) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Elimina todas las filas del dataframe que coincidan con los valores especificados\n",
        "        de brand, supermarket, variant y/o pack.size.\n",
        "        \n",
        "        La funci√≥n elimina TODAS las combinaciones que incluyan los valores especificados.\n",
        "        Por ejemplo, si pasas brand='brand-35' y supermarket='supermarket-A', eliminar√°\n",
        "        TODAS las series que tengan esa brand Y ese supermarket, sin importar variant o pack.size.\n",
        "        \n",
        "        \"\"\"\n",
        "        df_clean = df.copy()\n",
        "        \n",
        "        # Crear m√°scara inicial (todas las filas)\n",
        "        mask = pd.Series([True] * len(df_clean), index=df_clean.index)\n",
        "        \n",
        "        # Construir descripci√≥n del filtro\n",
        "        filter_parts = []\n",
        "        \n",
        "        # Aplicar filtros seg√∫n los par√°metros proporcionados\n",
        "        if brand is not None:\n",
        "            mask &= (df_clean['brand'] == brand)\n",
        "            filter_parts.append(f\"brand='{brand}'\")\n",
        "        \n",
        "        if supermarket is not None:\n",
        "            mask &= (df_clean['supermarket'] == supermarket)\n",
        "            filter_parts.append(f\"supermarket='{supermarket}'\")\n",
        "        \n",
        "        if variant is not None:\n",
        "            mask &= (df_clean['variant'] == variant)\n",
        "            filter_parts.append(f\"variant='{variant}'\")\n",
        "        \n",
        "        if pack_size is not None:\n",
        "            mask &= (df_clean['pack.size'] == pack_size)\n",
        "            filter_parts.append(f\"pack.size='{pack_size}'\")\n",
        "        \n",
        "        # Verificar que se haya especificado al menos un filtro\n",
        "        if not filter_parts:\n",
        "            if verbose:\n",
        "                print(\"‚ö†Ô∏è  No se especific√≥ ning√∫n filtro. No se elimin√≥ ninguna fila.\")\n",
        "            return df_clean\n",
        "        \n",
        "        # Contar filas a eliminar\n",
        "        rows_to_remove = mask.sum()\n",
        "        \n",
        "        if verbose:\n",
        "            print(\"=\" * 60)\n",
        "            print(\"üóëÔ∏è  ELIMINACI√ìN DE SERIES CONFLICTIVAS\")\n",
        "            print(\"=\" * 60)\n",
        "            print(f\"Filas iniciales: {len(df_clean):,}\")\n",
        "            print(f\"Filtro aplicado: {' & '.join(filter_parts)}\")\n",
        "            print()\n",
        "            \n",
        "            if rows_to_remove > 0:\n",
        "                # Obtener informaci√≥n adicional sobre lo que se elimina\n",
        "                affected_series = df_clean[mask]\n",
        "                \n",
        "                print(f\"üìä RESULTADOS:\")\n",
        "                print(f\"   ‚Üí Filas eliminadas: {rows_to_remove:,}\")\n",
        "            else:\n",
        "                print(\"‚ö†Ô∏è  No se encontraron filas que coincidan con el filtro especificado.\")\n",
        "            \n",
        "            print(\"=\" * 60)\n",
        "        \n",
        "        # Eliminar filas\n",
        "        df_clean = df_clean[~mask]\n",
        "        \n",
        "        return df_clean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "4a6e6a51",
      "metadata": {},
      "outputs": [],
      "source": [
        "if False:\n",
        "    conflictive_series_to_remove = [\n",
        "        {\"brand\": \"brand-14\", \"supermarket\": \"supermarket-A\", \"variant\": \"light\",\"pack_size\": \"351 - 500 GR\"},\n",
        "        {\"brand\": \"brand-14\", \"supermarket\": \"supermarket-A\", \"variant\": \"vegan\", \"pack_size\": \"701 - 1000 GR\"},\n",
        "        {\"brand\": \"brand-14\", \"supermarket\": \"supermarket-B\", \"variant\": \"light\", \"pack_size\": \"351 - 500 GR\"},\n",
        "\n",
        "        {\"brand\": \"brand-15\", \"supermarket\": \"supermarket-B\", \"variant\": \"light\", \"pack_size\": \"0 - 350 GR\"},\n",
        "        {\"brand\": \"brand-15\", \"supermarket\": \"supermarket-C\", \"variant\": \"light\", \"pack_size\": \"0 - 350 GR\"},\n",
        "\n",
        "        {\"brand\": \"brand-35\", \"supermarket\": \"supermarket-A\", \"variant\": \"light\", \"pack_size\": \"701 - 1000 GR\"},\n",
        "        {\"brand\": \"brand-35\", \"supermarket\": \"supermarket-C\", \"variant\": \"light\", \"pack_size\": \"0 - 350 GR\"},\n",
        "        {\"brand\": \"brand-35\", \"supermarket\": \"supermarket-D\", \"variant\": \"standard\", \"pack_size\": \"0 - 350 GR\"},\n",
        "\n",
        "    ]\n",
        "\n",
        "    for filtro in conflictive_series_to_remove:\n",
        "        data_clean = remove_conflictive_series(\n",
        "            data_clean,\n",
        "            brand=filtro.get(\"brand\"),\n",
        "            supermarket=filtro.get(\"supermarket\"),\n",
        "            variant=filtro.get(\"variant\"),\n",
        "            pack_size=filtro.get(\"pack_size\"),\n",
        "            verbose=True\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c14b128",
      "metadata": {},
      "source": [
        "#### RUN preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "43acde7e",
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_preprocessing(ARIMA_model: bool = False):\n",
        "\n",
        "    data_clean = clean_data(data)\n",
        "\n",
        "    distributions_variables(data_clean)\n",
        "    \n",
        "    if ARIMA_model:\n",
        "        filter_data = data_clean[   \n",
        "            (data_clean[\"brand\"] == \"brand-14\")\n",
        "            & (data_clean[\"supermarket\"] == \"supermarket-A\")\n",
        "            & (data_clean[\"variant\"] == \"light\")\n",
        "            & (data_clean[\"pack.size\"] == \"701 - 1000 GR\")\n",
        "        ].copy()\n",
        "\n",
        "        \n",
        "    else:\n",
        "        filter_data = None\n",
        "\n",
        "\n",
        "    # Split train/test por tiempo (basado en fecha, no en √≠ndice)\n",
        "    # Asegurar que date est√© en formato datetime\n",
        "    if ARIMA_model:\n",
        "        filter_data = filter_data.copy()\n",
        "        filter_data['date'] = pd.to_datetime(filter_data['date'])\n",
        "        # Ordenar por fecha para asegurar orden temporal\n",
        "        filter_data = filter_data.sort_values('date').reset_index(drop=True)\n",
        "        \n",
        "        # Calcular fecha de corte (80% del rango temporal)\n",
        "        date_min = filter_data['date'].min()\n",
        "        date_max = filter_data['date'].max()\n",
        "        date_range = date_max - date_min\n",
        "        train_cutoff = date_min + date_range * 0.8\n",
        "        \n",
        "        train_data = filter_data[filter_data['date'] <= train_cutoff].copy()\n",
        "        test_data = filter_data[filter_data['date'] > train_cutoff].copy()\n",
        "    else:\n",
        "        # Trabajar con copia para no modificar el dataframe original\n",
        "        data_work = data_clean.copy()\n",
        "        data_work['date'] = pd.to_datetime(data_work['date'])\n",
        "        # Ordenar por fecha para asegurar orden temporal\n",
        "        data_work = data_work.sort_values('date').reset_index(drop=True)\n",
        "        \n",
        "        # Calcular fecha de corte (80% del rango temporal)\n",
        "        date_min = data_work['date'].min()\n",
        "        date_max = data_work['date'].max()\n",
        "        date_range = date_max - date_min\n",
        "        train_cutoff = date_min + date_range * 0.8\n",
        "        \n",
        "        train_data = data_work[data_work['date'] <= train_cutoff].copy()\n",
        "        test_data = data_work[data_work['date'] > train_cutoff].copy()\n",
        "\n",
        "    # Box-Cox: fit solo en train\n",
        "    y_train_boxcox, boxcox_transformation_info = boxcox_transform(\n",
        "        train_data[\"volume.sales\"].copy()\n",
        "    )\n",
        "\n",
        "    # Aplicar misma transformaci√≥n a test\n",
        "    y_test = test_data[\"volume.sales\"].copy()\n",
        "    y_test_shifted = y_test + boxcox_transformation_info[\"constant\"]\n",
        "\n",
        "    if (y_test_shifted <= 0).any():\n",
        "        raise ValueError(\n",
        "            \"Box-Cox requiere valores positivos. Tras aplicar el shift aprendido en train, \"\n",
        "            \"test sigue teniendo valores <= 0. Revisa la serie o la estrategia de shift.\"\n",
        "        )\n",
        "\n",
        "    y_test_boxcox = stats.boxcox(\n",
        "        y_test_shifted, lmbda=boxcox_transformation_info[\"lambda\"]\n",
        "    )\n",
        "\n",
        "    return (\n",
        "        data_clean,\n",
        "        filter_data,\n",
        "        train_data,\n",
        "        test_data,\n",
        "        y_train_boxcox,\n",
        "        y_test_boxcox,\n",
        "        boxcox_transformation_info,\n",
        "    )"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "TFG",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
