{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed19a7d7",
   "metadata": {},
   "source": [
    "### 1.Processing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "229d1852",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/oscarjimenezbou/Documents/TFG_ADE/code/TFG/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Processing Data\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from utilities import *\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "raw_data = pd.read_excel(\"../data/Datos_Market_copy.xlsx\")\n",
    "\n",
    "sa = SalesAnalysis(raw_data)\n",
    "\n",
    "data = sa.data[sa.brand35]\n",
    "data = data.drop(columns=['unit.sales'])\n",
    "\n",
    "train_data, test_data = sa.divide_data_for_train_and_test(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668ce2ec",
   "metadata": {},
   "source": [
    "### 2. Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f260b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:           volume_sales   R-squared:                       0.833\n",
      "Model:                            OLS   Adj. R-squared:                  0.826\n",
      "Method:                 Least Squares   F-statistic:                     134.2\n",
      "Date:                Tue, 25 Nov 2025   Prob (F-statistic):          4.22e-223\n",
      "Time:                        19:11:24   Log-Likelihood:                -7343.0\n",
      "No. Observations:                 644   AIC:                         1.473e+04\n",
      "Df Residuals:                     620   BIC:                         1.484e+04\n",
      "Df Model:                          23                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "=================================================================================================================================\n",
      "                                                                    coef    std err          t      P>|t|      [0.025      0.975]\n",
      "---------------------------------------------------------------------------------------------------------------------------------\n",
      "Intercept                                                       1.21e+04   2671.315      4.530      0.000    6855.607    1.73e+04\n",
      "C(supermarket)[T.supermarket-B]                                2.319e+04   9730.122      2.383      0.017    4078.902    4.23e+04\n",
      "C(supermarket)[T.supermarket-D]                               -1.051e+05   1.42e+04     -7.402      0.000   -1.33e+05   -7.72e+04\n",
      "C(variant)[T.standard]                                         2.624e+04   6175.874      4.249      0.000    1.41e+04    3.84e+04\n",
      "C(variant)[T.vegan]                                            7.521e+04   1.85e+04      4.071      0.000    3.89e+04    1.11e+05\n",
      "C(pack_size)[T.351 - 500 GR]                                    8.22e+04   5413.805     15.183      0.000    7.16e+04    9.28e+04\n",
      "C(pack_size)[T.501 - 700 GR]                                   2.179e+04   3537.727      6.160      0.000    1.48e+04    2.87e+04\n",
      "C(pack_size)[T.701 - 1000 GR]                                 -1.993e+04   6573.311     -3.033      0.003   -3.28e+04   -7025.422\n",
      "C(supermarket)[T.supermarket-B]:C(variant)[T.light]           -1.924e+04   7704.010     -2.497      0.013   -3.44e+04   -4109.905\n",
      "C(supermarket)[T.supermarket-C]:C(variant)[T.light]            3.004e+04   6014.788      4.995      0.000    1.82e+04    4.19e+04\n",
      "C(supermarket)[T.supermarket-B]:C(variant)[T.standard]        -5.204e+04   7152.188     -7.276      0.000   -6.61e+04    -3.8e+04\n",
      "C(supermarket)[T.supermarket-C]:C(variant)[T.standard]        -3.216e+04   5178.112     -6.210      0.000   -4.23e+04    -2.2e+04\n",
      "C(supermarket)[T.supermarket-D]:C(variant)[T.standard]         6.913e+04   7708.818      8.968      0.000     5.4e+04    8.43e+04\n",
      "C(supermarket)[T.supermarket-B]:C(variant)[T.vegan]            7.112e+04   1.43e+04      4.988      0.000    4.31e+04    9.91e+04\n",
      "C(supermarket)[T.supermarket-C]:C(pack_size)[T.351 - 500 GR]  -3.356e+04   5564.661     -6.031      0.000   -4.45e+04   -2.26e+04\n",
      "C(supermarket)[T.supermarket-D]:C(pack_size)[T.351 - 500 GR]   3.171e+04   7293.879      4.348      0.000    1.74e+04     4.6e+04\n",
      "C(supermarket)[T.supermarket-D]:C(pack_size)[T.501 - 700 GR]   2.179e+04   3537.727      6.160      0.000    1.48e+04    2.87e+04\n",
      "C(supermarket)[T.supermarket-B]:C(pack_size)[T.701 - 1000 GR]  5.548e+04   7817.231      7.097      0.000    4.01e+04    7.08e+04\n",
      "C(variant)[T.light]:C(pack_size)[T.351 - 500 GR]               3.515e+04   9389.189      3.744      0.000    1.67e+04    5.36e+04\n",
      "C(variant)[T.vegan]:C(pack_size)[T.351 - 500 GR]               9.395e-11   1.29e-11      7.287      0.000    6.86e-11    1.19e-10\n",
      "C(variant)[T.light]:C(pack_size)[T.501 - 700 GR]               2.179e+04   3537.727      6.160      0.000    1.48e+04    2.87e+04\n",
      "C(variant)[T.light]:C(pack_size)[T.701 - 1000 GR]              2.423e+04   6821.642      3.552      0.000    1.08e+04    3.76e+04\n",
      "C(variant)[T.standard]:C(pack_size)[T.701 - 1000 GR]          -4.416e+04   7450.494     -5.928      0.000   -5.88e+04   -2.95e+04\n",
      "price:C(supermarket)[T.supermarket-B]                         -3.204e+04   8711.473     -3.677      0.000   -4.91e+04   -1.49e+04\n",
      "price:C(supermarket)[T.supermarket-D]                          1.217e+05   1.45e+04      8.375      0.000    9.32e+04     1.5e+05\n",
      "price:C(variant)[T.light]                                     -6.512e+04   9743.408     -6.684      0.000   -8.43e+04    -4.6e+04\n",
      "price:C(variant)[T.vegan]                                      -8.96e+04      2e+04     -4.477      0.000   -1.29e+05   -5.03e+04\n",
      "price:C(pack_size)[T.701 - 1000 GR]                            4.872e+04   8836.075      5.513      0.000    3.14e+04    6.61e+04\n",
      "==============================================================================\n",
      "Omnibus:                      108.327   Durbin-Watson:                   1.898\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1530.123\n",
      "Skew:                           0.176   Prob(JB):                         0.00\n",
      "Kurtosis:                      10.543   Cond. No.                     1.47e+16\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The smallest eigenvalue is 6.43e-30. This might indicate that there are\n",
      "strong multicollinearity problems or that the design matrix is singular.\n"
     ]
    }
   ],
   "source": [
    "# Linear Regression\n",
    "model, design_info, selected_columns = sa.modelization_with_backward_elimination(train_data)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efd3384",
   "metadata": {},
   "source": [
    "### 3. Create exogenous variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236fb62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating exogenous variables\n",
    "X_train_exog = sa.x_train_exog(train_data, selected_columns, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32cf27f",
   "metadata": {},
   "source": [
    "Diagnostic exogenous variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4592a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DIAGN√ìSTICO DE VARIABLES EX√ìGENAS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Informaci√≥n b√°sica\n",
    "print(\"\\n1. INFORMACI√ìN B√ÅSICA:\")\n",
    "print(f\"   - N√∫mero de observaciones: {len(X_train_exog)}\")\n",
    "print(f\"   - N√∫mero de variables ex√≥genas: {len(X_train_exog.columns)}\")\n",
    "print(\"   - Nombres de variables ex√≥genas:\")\n",
    "for col in X_train_exog.columns:\n",
    "    print(f\"     ¬∑ {col}\")\n",
    "\n",
    "# 2. Valores faltantes\n",
    "print(\"\\n2. VALORES FALTANTES:\")\n",
    "missing = X_train_exog.isna().sum()\n",
    "if missing.sum() > 0:\n",
    "    print(\"   ‚ö†Ô∏è  HAY VALORES FALTANTES:\")\n",
    "    print(missing[missing > 0])\n",
    "else:\n",
    "    print(\"   ‚úÖ No hay valores faltantes\")\n",
    "\n",
    "# 3. Valores infinitos\n",
    "print(\"\\n3. VALORES INFINITOS:\")\n",
    "inf_values = np.isinf(X_train_exog).sum()\n",
    "if inf_values.sum() > 0:\n",
    "    print(\"   ‚ö†Ô∏è  HAY VALORES INFINITOS:\")\n",
    "    print(inf_values[inf_values > 0])\n",
    "else:\n",
    "    print(\"   ‚úÖ No hay valores infinitos\")\n",
    "\n",
    "# 4. Estad√≠sticas descriptivas\n",
    "print(\"\\n4. ESTAD√çSTICAS DESCRIPTIVAS:\")\n",
    "print(X_train_exog.describe())\n",
    "\n",
    "# 5. Valores extremos (outliers)\n",
    "print(\"\\n5. DETECCI√ìN DE VALORES EXTREMOS:\")\n",
    "Q1 = X_train_exog.quantile(0.25)\n",
    "Q3 = X_train_exog.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "outliers = ((X_train_exog < (Q1 - 3 * IQR)) | (X_train_exog > (Q3 + 3 * IQR))).sum()\n",
    "if outliers.sum() > 0:\n",
    "    print(\"   ‚ö†Ô∏è  Variables con valores extremos (>3*IQR):\")\n",
    "    print(outliers[outliers > 0])\n",
    "else:\n",
    "    print(\"   ‚úÖ No se detectaron valores extremos significativos\")\n",
    "\n",
    "# 6. Rango de valores (escalas muy diferentes)\n",
    "print(\"\\n6. RANGOS DE VALORES (verificar escalas):\")\n",
    "ranges = X_train_exog.max() - X_train_exog.min()\n",
    "print(ranges.sort_values(ascending=False))\n",
    "print(\"\\n   ‚ö†Ô∏è  Si hay rangos muy diferentes, considera estandarizar las variables\")\n",
    "\n",
    "# 7. MATRIZ DE CORRELACI√ìN (multicolinealidad)\n",
    "print(\"\\n7. AN√ÅLISIS DE MULTICOLINEALIDAD:\")\n",
    "corr_matrix = X_train_exog.corr()\n",
    "high_corr = []\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i+1, len(corr_matrix.columns)):\n",
    "        if abs(corr_matrix.iloc[i, j]) > 0.95:  # Correlaci√≥n muy alta\n",
    "            high_corr.append((corr_matrix.columns[i], corr_matrix.columns[j], corr_matrix.iloc[i, j]))\n",
    "\n",
    "if high_corr:\n",
    "    print(\"   ‚ö†Ô∏è  VARIABLES ALTAMENTE CORRELACIONADAS (|r| > 0.95):\")\n",
    "    for var1, var2, corr_val in high_corr:\n",
    "        print(f\"      {var1} <-> {var2}: {corr_val:.4f}\")\n",
    "else:\n",
    "    print(\"   ‚úÖ No se encontraron correlaciones extremadamente altas\")\n",
    "\n",
    "# 8. N√∫mero de condici√≥n (medida de multicolinealidad)\n",
    "print(\"\\n8. N√öMERO DE CONDICI√ìN (multicolinealidad):\")\n",
    "try:\n",
    "    X_matrix = X_train_exog.values\n",
    "    # A√±adir constante para el c√°lculo\n",
    "    X_with_const = np.column_stack([np.ones(len(X_matrix)), X_matrix])\n",
    "    cond_number = np.linalg.cond(X_with_const)\n",
    "    print(f\"   N√∫mero de condici√≥n: {cond_number:.2e}\")\n",
    "    if cond_number > 1e12:\n",
    "        print(\"   ‚ö†Ô∏è  MULTICOLINEALIDAD EXTREMA - Esto puede causar AIC infinito\")\n",
    "    elif cond_number > 1e8:\n",
    "        print(\"   ‚ö†Ô∏è  Multicolinealidad alta - Puede causar problemas num√©ricos\")\n",
    "    else:\n",
    "        print(\"   ‚úÖ N√∫mero de condici√≥n aceptable\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Error calculando n√∫mero de condici√≥n: {e}\")\n",
    "\n",
    "# 9. Verificar si hay columnas constantes o casi constantes\n",
    "print(\"\\n9. VARIABLES CONSTANTES O CASI CONSTANTES:\")\n",
    "constant_vars = []\n",
    "for col in X_train_exog.columns:\n",
    "    if X_train_exog[col].nunique() <= 1:\n",
    "        constant_vars.append(col)\n",
    "    elif X_train_exog[col].std() < 1e-10:\n",
    "        constant_vars.append(col)\n",
    "\n",
    "if constant_vars:\n",
    "    print(\"   ‚ö†Ô∏è  VARIABLES CONSTANTES O CASI CONSTANTES:\")\n",
    "    for var in constant_vars:\n",
    "        print(f\"      {var}: std={X_train_exog[var].std():.2e}, unique={X_train_exog[var].nunique()}\")\n",
    "else:\n",
    "    print(\"   ‚úÖ No hay variables constantes\")\n",
    "\n",
    "# 10. Verificar dimensiones con la variable dependiente\n",
    "print(\"\\n10. COMPATIBILIDAD CON VARIABLE DEPENDIENTE:\")\n",
    "print(f\"   - Longitud de y (volume.sales): {len(train_data['volume.sales'])}\")\n",
    "print(f\"   - Longitud de X_train_exog: {len(X_train_exog)}\")\n",
    "if len(train_data['volume.sales']) != len(X_train_exog):\n",
    "    print(\"   ‚ö†Ô∏è  DIMENSIONES NO COINCIDEN - Esto causar√° errores\")\n",
    "else:\n",
    "    print(\"   ‚úÖ Las dimensiones coinciden\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FIN DEL DIAGN√ìSTICO\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849eb364",
   "metadata": {},
   "source": [
    "Delete Problem Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fa9625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== SOLUCI√ìN: ELIMINAR VARIABLES PROBLEM√ÅTICAS ==========\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SOLUCI√ìN: LIMPIEZA DE VARIABLES EX√ìGENAS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Crear una copia para trabajar\n",
    "X_train_exog_clean = X_train_exog.copy()\n",
    "\n",
    "# 1. Identificar y eliminar variables constantes (rango = 0)\n",
    "print(\"\\n1. ELIMINANDO VARIABLES CONSTANTES:\")\n",
    "ranges = X_train_exog_clean.max() - X_train_exog_clean.min()\n",
    "constant_vars = ranges[ranges == 0].index.tolist()\n",
    "if constant_vars:\n",
    "    print(f\"   Eliminando {len(constant_vars)} variables constantes:\")\n",
    "    for var in constant_vars:\n",
    "        print(f\"     - {var}\")\n",
    "    X_train_exog_clean = X_train_exog_clean.drop(columns=constant_vars)\n",
    "else:\n",
    "    print(\"   ‚úÖ No hay variables constantes\")\n",
    "\n",
    "# 2. Identificar y eliminar variables con correlaci√≥n perfecta (|r| = 1.0)\n",
    "print(\"\\n2. ELIMINANDO VARIABLES CON CORRELACI√ìN PERFECTA:\")\n",
    "corr_matrix = X_train_exog_clean.corr()\n",
    "perfect_corr_pairs = []\n",
    "vars_to_remove = set()\n",
    "\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i+1, len(corr_matrix.columns)):\n",
    "        if abs(corr_matrix.iloc[i, j]) >= 1.0 - 1e-10:  # Correlaci√≥n perfecta (con tolerancia num√©rica)\n",
    "            var1 = corr_matrix.columns[i]\n",
    "            var2 = corr_matrix.columns[j]\n",
    "            perfect_corr_pairs.append((var1, var2, corr_matrix.iloc[i, j]))\n",
    "            # Eliminar la segunda variable (mantener la primera)\n",
    "            if var2 not in vars_to_remove:\n",
    "                vars_to_remove.add(var2)\n",
    "\n",
    "if vars_to_remove:\n",
    "    print(f\"   Eliminando {len(vars_to_remove)} variables con correlaci√≥n perfecta:\")\n",
    "    for var in vars_to_remove:\n",
    "        print(f\"     - {var}\")\n",
    "    X_train_exog_clean = X_train_exog_clean.drop(columns=list(vars_to_remove))\n",
    "else:\n",
    "    print(\"   ‚úÖ No hay variables con correlaci√≥n perfecta\")\n",
    "\n",
    "# 3. Limpieza adicional: eliminar variables altamente correlacionadas (|r| > 0.95)\n",
    "# Esto ayuda a reducir a√∫n m√°s la multicolinealidad\n",
    "print(\"\\n3. ELIMINANDO VARIABLES ALTAMENTE CORRELACIONADAS (|r| > 0.95):\")\n",
    "corr_matrix = X_train_exog_clean.corr()\n",
    "high_corr_vars = set()\n",
    "high_corr_pairs = []\n",
    "\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i+1, len(corr_matrix.columns)):\n",
    "        if abs(corr_matrix.iloc[i, j]) > 0.95:\n",
    "            var1 = corr_matrix.columns[i]\n",
    "            var2 = corr_matrix.columns[j]\n",
    "            high_corr_pairs.append((var1, var2, corr_matrix.iloc[i, j]))\n",
    "            # Eliminar la segunda variable (mantener la primera)\n",
    "            if var2 not in high_corr_vars:\n",
    "                high_corr_vars.add(var2)\n",
    "\n",
    "if high_corr_vars:\n",
    "    print(f\"   Eliminando {len(high_corr_vars)} variables altamente correlacionadas:\")\n",
    "    for var in high_corr_vars:\n",
    "        print(f\"     - {var}\")\n",
    "    X_train_exog_clean = X_train_exog_clean.drop(columns=list(high_corr_vars))\n",
    "else:\n",
    "    print(\"   ‚úÖ No hay variables altamente correlacionadas\")\n",
    "\n",
    "# 4. Verificar n√∫mero de condici√≥n despu√©s de la limpieza\n",
    "print(\"\\n4. VERIFICANDO N√öMERO DE CONDICI√ìN DESPU√âS DE LA LIMPIEZA:\")\n",
    "try:\n",
    "    X_matrix_clean = X_train_exog_clean.values\n",
    "    X_with_const_clean = np.column_stack([np.ones(len(X_matrix_clean)), X_matrix_clean])\n",
    "    cond_number_clean = np.linalg.cond(X_with_const_clean)\n",
    "    print(f\"   N√∫mero de condici√≥n original: 7.48e+17\")\n",
    "    print(f\"   N√∫mero de condici√≥n despu√©s de limpieza: {cond_number_clean:.2e}\")\n",
    "    \n",
    "    if cond_number_clean > 1e12:\n",
    "        print(\"   ‚ö†Ô∏è  A√öN HAY MULTICOLINEALIDAD EXTREMA\")\n",
    "        print(\"   üí° Considera eliminar m√°s variables o usar regularizaci√≥n\")\n",
    "    elif cond_number_clean > 1e8:\n",
    "        print(\"   ‚ö†Ô∏è  Multicolinealidad alta - Puede causar problemas num√©ricos\")\n",
    "        print(\"   üí° El modelo puede funcionar, pero algunos ajustes pueden fallar\")\n",
    "    elif cond_number_clean > 1e6:\n",
    "        print(\"   ‚ö†Ô∏è  Multicolinealidad moderada - Generalmente aceptable\")\n",
    "    else:\n",
    "        print(\"   ‚úÖ N√∫mero de condici√≥n mejorado significativamente\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Error: {e}\")\n",
    "\n",
    "# 5. Resumen final\n",
    "print(\"\\n5. RESUMEN:\")\n",
    "print(f\"   - Variables originales: {len(X_train_exog.columns)}\")\n",
    "print(f\"   - Variables despu√©s de limpieza: {len(X_train_exog_clean.columns)}\")\n",
    "print(f\"   - Variables eliminadas: {len(X_train_exog.columns) - len(X_train_exog_clean.columns)}\")\n",
    "print(f\"   - Reducci√≥n: {((len(X_train_exog.columns) - len(X_train_exog_clean.columns)) / len(X_train_exog.columns) * 100):.1f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"VARIABLES LIMPIAS LISTAS PARA USAR\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nüí° Usa 'X_train_exog_clean' en lugar de 'X_train_exog' para auto_arima\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69c21c6",
   "metadata": {},
   "source": [
    "Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1da886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== VISUALIZACI√ìN: MATRIZ DE CORRELACI√ìN ==========\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"VISUALIZACI√ìN: MATRIZ DE CORRELACI√ìN\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Crear figura con dos subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "# Matriz de correlaci√≥n original\n",
    "corr_original = X_train_exog.corr()\n",
    "sns.heatmap(corr_original, annot=False, cmap='coolwarm', center=0, \n",
    "            square=True, fmt='.2f', cbar_kws={\"shrink\": 0.8}, ax=axes[0])\n",
    "axes[0].set_title('Matriz de Correlaci√≥n - Variables Originales', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xticklabels(axes[0].get_xticklabels(), rotation=45, ha='right', fontsize=8)\n",
    "axes[0].set_yticklabels(axes[0].get_yticklabels(), rotation=0, fontsize=8)\n",
    "\n",
    "# Matriz de correlaci√≥n despu√©s de limpieza\n",
    "corr_clean = X_train_exog_clean.corr()\n",
    "sns.heatmap(corr_clean, annot=False, cmap='coolwarm', center=0, \n",
    "            square=True, fmt='.2f', cbar_kws={\"shrink\": 0.8}, ax=axes[1])\n",
    "axes[1].set_title('Matriz de Correlaci√≥n - Variables Limpias', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xticklabels(axes[1].get_xticklabels(), rotation=45, ha='right', fontsize=8)\n",
    "axes[1].set_yticklabels(axes[1].get_yticklabels(), rotation=0, fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Estad√≠sticas de correlaci√≥n\n",
    "print(\"\\nüìä ESTAD√çSTICAS DE CORRELACI√ìN:\")\n",
    "print(f\"   Original: {len(corr_original)} variables\")\n",
    "print(f\"   Limpias: {len(corr_clean)} variables\")\n",
    "print(f\"\\n   Correlaciones altas (|r| > 0.9) en original: {((abs(corr_original) > 0.9).sum().sum() - len(corr_original)) // 2}\")\n",
    "print(f\"   Correlaciones altas (|r| > 0.9) en limpias: {((abs(corr_clean) > 0.9).sum().sum() - len(corr_clean)) // 2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30965cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"PROBANDO AUTO_ARIMA CON VARIABLES LIMPIAS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "auto_arima_model_clean = auto_arima( \n",
    "    y=train_data['volume.sales'],\n",
    "    X=X_train_exog_clean,  # Usar las variables limpias\n",
    "    start_p=0,\n",
    "    d=0,  \n",
    "    start_q=0,\n",
    "    max_p=3,  # Puedes aumentar esto si quieres probar m√°s modelos\n",
    "    max_q=3,\n",
    "    start_P=0,\n",
    "    D=1,\n",
    "    start_Q=0,\n",
    "    max_P=1, \n",
    "    max_Q=1,\n",
    "    m=12,\n",
    "    seasonal=True,\n",
    "    trace=True,\n",
    "    error_action=\"warn\",\n",
    "    suppress_warnings=True,\n",
    "    stepwise=True,  # Usar stepwise=True para ser m√°s eficiente\n",
    "    n_fits=50,  # N√∫mero de ajustes a probar\n",
    "    information_criterion='aic'\n",
    ")\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e97c481",
   "metadata": {},
   "source": [
    "### 4. Analysing Resids "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ccc4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "# An√°lisis de residuos\n",
    "\n",
    "residuals = auto_arima_model_clean.arima_res_.resid\n",
    "\n",
    "print(\"-----------------Residues Analysis (White Noise) -----------------\")\n",
    "sa.residual_white_noise_test(residuals)\n",
    "print(\"------------------------------------------------------------------\") \n",
    "\n",
    "# ========== GR√ÅFICO DE DIAGN√ìSTICO DE RESIDUOS ==========\n",
    "\n",
    "# Obtener residuos y valores ajustados\n",
    "residuals = auto_arima_model_clean.arima_res_.resid\n",
    "fitted_values = auto_arima_model_clean.arima_res_.fittedvalues\n",
    "\n",
    "# Crear figura con 6 subplots (3 filas x 2 columnas)\n",
    "fig, axes = plt.subplots(3, 2, figsize=(15, 10))\n",
    "fig.suptitle('An√°lisis de Residuos - Diagn√≥stico del Modelo ARIMAX', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Residuos vs. Tiempo (arriba-izquierda)\n",
    "axes[0, 0].plot(residuals, color='blue', linewidth=0.8)\n",
    "axes[0, 0].axhline(y=0, color='red', linestyle='--', linewidth=1)\n",
    "axes[0, 0].set_title('Residuos vs. Tiempo', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Index')\n",
    "axes[0, 0].set_ylabel('Residuos')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Residuos vs. Valores Ajustados (arriba-derecha)\n",
    "axes[0, 1].scatter(fitted_values, residuals, alpha=0.5, s=10, color='blue')\n",
    "axes[0, 1].axhline(y=0, color='red', linestyle='--', linewidth=1)\n",
    "axes[0, 1].set_title('Residuos vs. Valores Ajustados', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Fitted Values')\n",
    "axes[0, 1].set_ylabel('Residuos')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Q-Q Plot (medio-izquierda)\n",
    "stats.probplot(residuals, dist=\"norm\", plot=axes[1, 0])\n",
    "axes[1, 0].set_title('Q-Q Plot (Normalidad)', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. ACF (Autocorrelaci√≥n) (medio-derecha)\n",
    "plot_acf(residuals, lags=12, ax=axes[1, 1], alpha=0.05)\n",
    "axes[1, 1].set_title('Autocorrelaci√≥n (ACF)', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 5. PACF (Autocorrelaci√≥n Parcial) (abajo-izquierda)\n",
    "plot_pacf(residuals, lags=12, ax=axes[2, 0], alpha=0.05, method='ywm')\n",
    "axes[2, 0].set_title('Autocorrelaci√≥n Parcial (PACF)', fontsize=12, fontweight='bold')\n",
    "axes[2, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Histograma de Residuos (abajo-derecha)\n",
    "axes[2, 1].hist(residuals, bins=30, edgecolor='black', alpha=0.7, color='blue')\n",
    "axes[2, 1].set_title('Histograma de Residuos', fontsize=12, fontweight='bold')\n",
    "axes[2, 1].set_xlabel('Residuos')\n",
    "axes[2, 1].set_ylabel('Frequency')\n",
    "axes[2, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e873f513",
   "metadata": {},
   "outputs": [],
   "source": [
    " #TODO: revisar resultado para ver que no hay outliers muy grandes.\n",
    "# Percentil 99\n",
    "umbral_p99 = residuals.abs().quantile(0.99)\n",
    "\n",
    "outliers_p99 = residuals[residuals.abs() > umbral_p99]\n",
    "\n",
    "print(f\"Residuales por encima del 99%: {len(outliers_p99)}\")\n",
    "# print data from outliers_p99\n",
    "print(outliers_p99)\n",
    "print(outliers_p99.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c78490",
   "metadata": {},
   "source": [
    "## Forecasting begins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef220d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== PREPARAR VARIABLES EX√ìGENAS DE TEST ==========\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PREPARACI√ìN DE VARIABLES EX√ìGENAS PARA TEST\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Preparar variables ex√≥genas de test usando la misma transformaci√≥n que en train\n",
    "X_test_exog = sa.x_test_exog(test_data, selected_columns, design_info)\n",
    "\n",
    "print(f\"\\n‚úÖ Variables ex√≥genas de test preparadas:\")\n",
    "print(f\"   - N√∫mero de observaciones: {len(X_test_exog)}\")\n",
    "print(f\"   - N√∫mero de variables ex√≥genas: {len(X_test_exog.columns)}\")\n",
    "print(f\"   - Columnas: {list(X_test_exog.columns)}\")\n",
    "\n",
    "# Verificar dimensiones\n",
    "print(f\"\\nüìä Verificaci√≥n de dimensiones:\")\n",
    "print(f\"   - Train data length: {len(train_data)}\")\n",
    "print(f\"   - Test data length: {len(test_data)}\")\n",
    "print(f\"   - X_train_exog original: {len(X_train_exog)} observaciones, {len(X_train_exog.columns)} variables\")\n",
    "print(f\"   - X_test_exog: {len(X_test_exog)} observaciones, {len(X_test_exog.columns)} variables\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8dd272c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== LIMPIAR VARIABLES EX√ìGENAS DE TEST ==========\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"LIMPIEZA DE VARIABLES EX√ìGENAS DE TEST\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Limpiar variables ex√≥genas de test usando la misma funci√≥n\n",
    "X_test_exog_clean_temp, removed_test, cond_before_test, cond_after_test = sa.clean_exogenous_variables(\n",
    "    X_test_exog, corr_threshold=0.95, verbose=True\n",
    ")\n",
    "\n",
    "# IMPORTANTE: Filtrar columnas para que coincidan exactamente con las de train\n",
    "# Solo mantener las columnas que est√°n en X_train_exog_clean\n",
    "train_columns = set(X_train_exog_clean.columns)\n",
    "test_columns_available = set(X_test_exog_clean_temp.columns)\n",
    "\n",
    "# Columnas que est√°n en train pero no en test (despu√©s de limpieza)\n",
    "missing_in_test = train_columns - test_columns_available\n",
    "# Columnas que est√°n en test pero no en train\n",
    "extra_in_test = test_columns_available - train_columns\n",
    "\n",
    "print(f\"\\nüîç An√°lisis de compatibilidad de columnas:\")\n",
    "print(f\"   - Columnas en train (limpias): {len(train_columns)}\")\n",
    "print(f\"   - Columnas en test (despu√©s de limpieza): {len(test_columns_available)}\")\n",
    "if missing_in_test:\n",
    "    print(f\"   ‚ö†Ô∏è  Columnas en train pero NO en test: {len(missing_in_test)}\")\n",
    "    for col in sorted(missing_in_test):\n",
    "        print(f\"      ¬∑ {col}\")\n",
    "if extra_in_test:\n",
    "    print(f\"   ‚ö†Ô∏è  Columnas en test pero NO en train: {len(extra_in_test)}\")\n",
    "    for col in sorted(extra_in_test):\n",
    "        print(f\"      ¬∑ {col}\")\n",
    "\n",
    "# Asegurar que X_test_exog_clean tenga EXACTAMENTE las mismas columnas que X_train_exog_clean\n",
    "# En el mismo orden y con las columnas faltantes rellenadas con ceros\n",
    "X_test_exog_clean = pd.DataFrame(index=X_test_exog_clean_temp.index)\n",
    "\n",
    "# Para cada columna en train, agregarla desde test (si existe) o con ceros (si no existe)\n",
    "for col in X_train_exog_clean.columns:\n",
    "    if col in X_test_exog_clean_temp.columns:\n",
    "        # La columna existe en test, usar sus valores\n",
    "        X_test_exog_clean[col] = X_test_exog_clean_temp[col]\n",
    "    else:\n",
    "        # La columna NO existe en test, agregarla con ceros\n",
    "        X_test_exog_clean[col] = 0.0\n",
    "        print(f\"   ‚ö†Ô∏è  Columna '{col}' no encontrada en test, se agregar√° con valores 0\")\n",
    "\n",
    "# Asegurar que las columnas est√©n en el mismo orden que en train\n",
    "X_test_exog_clean = X_test_exog_clean[X_train_exog_clean.columns]\n",
    "\n",
    "# Verificar que las columnas coincidan\n",
    "if list(X_test_exog_clean.columns) == list(X_train_exog_clean.columns):\n",
    "    print(f\"\\n‚úÖ Columnas de test coinciden PERFECTAMENTE con train\")\n",
    "    print(f\"   - Columnas finales: {len(X_test_exog_clean.columns)}\")\n",
    "    print(f\"   - Columnas agregadas con ceros: {len(missing_in_test)}\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  ALERTA: Las columnas NO coinciden completamente\")\n",
    "    print(f\"   - Columnas en train: {list(X_train_exog_clean.columns)}\")\n",
    "    print(f\"   - Columnas en test: {list(X_test_exog_clean.columns)}\")\n",
    "\n",
    "print(f\"\\nüìä Resumen final:\")\n",
    "print(f\"   - Variables ex√≥genas de test (limpias y filtradas): {len(X_test_exog_clean.columns)}\")\n",
    "print(f\"   - Observaciones de test: {len(X_test_exog_clean)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7136d866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== VERIFICAR COMPATIBILIDAD FINAL ==========\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"VERIFICACI√ìN FINAL DE COMPATIBILIDAD\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Verificar que no haya valores faltantes o infinitos\n",
    "print(\"\\n1. VALORES FALTANTES E INFINITOS:\")\n",
    "missing_test = X_test_exog_clean.isna().sum().sum()\n",
    "inf_test = np.isinf(X_test_exog_clean).sum().sum()\n",
    "\n",
    "if missing_test > 0:\n",
    "    print(f\"   ‚ö†Ô∏è  HAY {missing_test} VALORES FALTANTES en X_test_exog_clean\")\n",
    "    print(X_test_exog_clean.isna().sum()[X_test_exog_clean.isna().sum() > 0])\n",
    "else:\n",
    "    print(f\"   ‚úÖ No hay valores faltantes\")\n",
    "\n",
    "if inf_test > 0:\n",
    "    print(f\"   ‚ö†Ô∏è  HAY {inf_test} VALORES INFINITOS en X_test_exog_clean\")\n",
    "else:\n",
    "    print(f\"   ‚úÖ No hay valores infinitos\")\n",
    "\n",
    "# Verificar que las dimensiones sean correctas\n",
    "print(\"\\n2. DIMENSIONES:\")\n",
    "print(f\"   - X_train_exog_clean: {X_train_exog_clean.shape}\")\n",
    "print(f\"   - X_test_exog_clean: {X_test_exog_clean.shape}\")\n",
    "print(f\"   - test_data['volume.sales']: {test_data['volume.sales'].shape}\")\n",
    "\n",
    "if len(test_data['volume.sales']) != len(X_test_exog_clean):\n",
    "    print(f\"   ‚ö†Ô∏è  ALERTA: Las dimensiones NO coinciden\")\n",
    "else:\n",
    "    print(f\"   ‚úÖ Las dimensiones coinciden\")\n",
    "\n",
    "# Verificar que las columnas sean id√©nticas\n",
    "print(\"\\n3. COLUMNAS:\")\n",
    "train_cols = set(X_train_exog_clean.columns)\n",
    "test_cols = set(X_test_exog_clean.columns)\n",
    "\n",
    "if train_cols == test_cols:\n",
    "    print(f\"   ‚úÖ Las columnas son ID√âNTICAS\")\n",
    "    print(f\"   - N√∫mero de columnas: {len(train_cols)}\")\n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è  Las columnas NO son id√©nticas\")\n",
    "    print(f\"   - Diferencia: {train_cols.symmetric_difference(test_cols)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"LISTO PARA HACER PREDICCIONES\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fde1afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== REALIZAR PREDICCIONES ==========\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PREDICCIONES CON MODELO GLOBAL ARIMAX\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüìä Informaci√≥n del modelo:\")\n",
    "print(f\"   - Orden ARIMA: {auto_arima_model_clean.order}\")\n",
    "print(f\"   - Orden estacional: {auto_arima_model_clean.seasonal_order}\")\n",
    "print(f\"   - AIC: {auto_arima_model_clean.aic():.2f}\")\n",
    "print(f\"   - Variables ex√≥genas: {len(X_train_exog_clean.columns)}\")\n",
    "print(f\"   - Pasos a predecir: {len(test_data)}\")\n",
    "\n",
    "print(\"\\n‚è≥ Realizando predicciones...\")\n",
    "\n",
    "try:\n",
    "    # Obtener el modelo statsmodels subyacente\n",
    "    arima_res = auto_arima_model_clean.arima_res_\n",
    "    \n",
    "    # Hacer predicciones con intervalos de confianza\n",
    "    forecast = arima_res.get_forecast(steps=len(test_data), exog=X_test_exog_clean)\n",
    "\n",
    "    # Extraer predicciones punto\n",
    "    predictions = forecast.predicted_mean\n",
    "    \n",
    "    # Extraer intervalos de confianza (opcional)\n",
    "    conf_int = forecast.conf_int()\n",
    "    \n",
    "    print(\"‚úÖ Predicciones realizadas exitosamente\")\n",
    "    print(f\"\\nüìà Resumen de predicciones:\")\n",
    "    print(f\"   - N√∫mero de predicciones: {len(predictions)}\")\n",
    "    print(f\"   - Media de predicciones: {predictions.mean():.2f}\")\n",
    "    print(f\"   - Desviaci√≥n est√°ndar: {predictions.std():.2f}\")\n",
    "    print(f\"   - M√≠nimo: {predictions.min():.2f}\")\n",
    "    print(f\"   - M√°ximo: {predictions.max():.2f}\")\n",
    "    \n",
    "    # Comparar con valores reales\n",
    "    actual_values = test_data['volume.sales'].values\n",
    "    print(f\"\\nüìä Comparaci√≥n con valores reales:\")\n",
    "    print(f\"   - Media real: {actual_values.mean():.2f}\")\n",
    "    print(f\"   - Media predicha: {predictions.mean():.2f}\")\n",
    "    print(f\"   - Diferencia: {abs(actual_values.mean() - predictions.mean()):.2f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error al realizar predicciones: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2748379c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== TABLA DE PREDICCIONES ==========\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TABLA DE PREDICCIONES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Verificar que las predicciones se hayan realizado correctamente\n",
    "if 'predictions' not in locals() or predictions is None:\n",
    "    print(\"‚ùå Error: Las predicciones no se realizaron correctamente.\")\n",
    "    print(\"   Por favor, ejecuta primero la celda anterior de predicciones.\")\n",
    "else:\n",
    "    # Preparar datos para la tabla\n",
    "    fecha_col = test_data.index if hasattr(test_data.index, 'strftime') else range(len(test_data))\n",
    "    if hasattr(test_data.index, 'strftime'):\n",
    "        try:\n",
    "            fecha_col = test_data.index\n",
    "        except:\n",
    "            fecha_col = range(len(test_data))\n",
    "    \n",
    "    # Crear diccionario base para el DataFrame\n",
    "    table_data = {\n",
    "        'Fecha': fecha_col,\n",
    "        'Valor_Real': test_data['volume.sales'].values,\n",
    "        'Predicci√≥n': predictions.values,\n",
    "        'Error': test_data['volume.sales'].values - predictions.values,\n",
    "        'Error_Absoluto': np.abs(test_data['volume.sales'].values - predictions.values),\n",
    "        'Error_Porcentual': ((test_data['volume.sales'].values - predictions.values) / \n",
    "                             np.maximum(1e-9, np.abs(test_data['volume.sales'].values))) * 100\n",
    "    }\n",
    "    \n",
    "    # Agregar intervalos de confianza si est√°n disponibles\n",
    "    if 'conf_int' in locals() and conf_int is not None:\n",
    "        table_data['Limite_Inferior_IC'] = conf_int.iloc[:, 0].values\n",
    "        table_data['Limite_Superior_IC'] = conf_int.iloc[:, 1].values\n",
    "    \n",
    "    # Crear DataFrame con los datos de predicci√≥n\n",
    "    predictions_df = pd.DataFrame(table_data)\n",
    "    \n",
    "    # Mostrar la tabla completa\n",
    "    print(\"\\nüìã Tabla completa de predicciones:\")\n",
    "    print(\"=\" * 60)\n",
    "    display(predictions_df)\n",
    "    \n",
    "    # Mostrar estad√≠sticas resumidas\n",
    "    print(\"\\nüìä Estad√≠sticas resumidas:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\nValores Reales:\")\n",
    "    print(f\"   - Media: {predictions_df['Valor_Real'].mean():.2f}\")\n",
    "    print(f\"   - Desviaci√≥n est√°ndar: {predictions_df['Valor_Real'].std():.2f}\")\n",
    "    print(f\"   - M√≠nimo: {predictions_df['Valor_Real'].min():.2f}\")\n",
    "    print(f\"   - M√°ximo: {predictions_df['Valor_Real'].max():.2f}\")\n",
    "    \n",
    "    print(f\"\\nPredicciones:\")\n",
    "    print(f\"   - Media: {predictions_df['Predicci√≥n'].mean():.2f}\")\n",
    "    print(f\"   - Desviaci√≥n est√°ndar: {predictions_df['Predicci√≥n'].std():.2f}\")\n",
    "    print(f\"   - M√≠nimo: {predictions_df['Predicci√≥n'].min():.2f}\")\n",
    "    print(f\"   - M√°ximo: {predictions_df['Predicci√≥n'].max():.2f}\")\n",
    "    \n",
    "    print(f\"\\nErrores:\")\n",
    "    print(f\"   - Error medio: {predictions_df['Error'].mean():.2f}\")\n",
    "    print(f\"   - Error absoluto medio (MAE): {predictions_df['Error_Absoluto'].mean():.2f}\")\n",
    "    print(f\"   - Error porcentual absoluto medio (MAPE): {predictions_df['Error_Porcentual'].abs().mean():.2f}%\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1922bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== IDENTIFICAR SERIES DISPONIBLES ==========\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"IDENTIFICACI√ìN DE SERIES DISPONIBLES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Obtener combinaciones √∫nicas de (supermarket, variant, pack.size) en los datos\n",
    "combinations = data[['supermarket', 'variant', 'pack.size']].drop_duplicates().sort_values(['supermarket', 'variant', 'pack.size'])\n",
    "\n",
    "print(f\"\\nüìä Combinaciones disponibles en los datos:\")\n",
    "print(f\"   - Total de combinaciones √∫nicas: {len(combinations)}\")\n",
    "print(f\"\\nüí° Series disponibles:\")\n",
    "\n",
    "for idx, row in combinations.iterrows():\n",
    "    print(f\"   {idx}: {row['supermarket']} | {row['variant']} | {row['pack.size']}\")\n",
    "\n",
    "# Verificar disponibilidad en train y test\n",
    "print(f\"\\nüîç Verificando disponibilidad en train_data y test_data:\")\n",
    "\n",
    "available_series = []\n",
    "\n",
    "for idx, row in combinations.iterrows():\n",
    "    train_filter = (\n",
    "        (train_data['supermarket'] == row['supermarket']) &\n",
    "        (train_data['variant'] == row['variant']) &\n",
    "        (train_data['pack.size'] == row['pack.size'])\n",
    "    )\n",
    "    test_filter = (\n",
    "        (test_data['supermarket'] == row['supermarket']) &\n",
    "        (test_data['variant'] == row['variant']) &\n",
    "        (test_data['pack.size'] == row['pack.size'])\n",
    "    )\n",
    "    \n",
    "    train_count = train_filter.sum()\n",
    "    test_count = test_filter.sum()\n",
    "    \n",
    "    if train_count > 0 and test_count > 0:\n",
    "        available_series.append({\n",
    "            'supermarket': row['supermarket'],\n",
    "            'variant': row['variant'],\n",
    "            'pack.size': row['pack.size'],\n",
    "            'train_count': train_count,\n",
    "            'test_count': test_count\n",
    "        })\n",
    "\n",
    "print(f\"\\n‚úÖ Series disponibles en AMBOS (train y test): {len(available_series)}\")\n",
    "for i, series in enumerate(available_series):\n",
    "    print(f\"   {i+1}. {series['supermarket']} | {series['variant']} | {series['pack.size']} (Train: {series['train_count']}, Test: {series['test_count']})\")\n",
    "\n",
    "# Seleccionar la primera serie disponible (o la m√°s com√∫n si hay varias)\n",
    "if available_series:\n",
    "    #TODO:\n",
    "    selected_series = available_series[0]  # Puedes cambiar esto para seleccionar otra serie\n",
    "    print(f\"\\nüéØ Serie seleccionada para visualizaci√≥n:\")\n",
    "    print(f\"   - Supermarket: {selected_series['supermarket']}\")\n",
    "    print(f\"   - Variant: {selected_series['variant']}\")\n",
    "    print(f\"   - Pack Size: {selected_series['pack.size']}\")\n",
    "    print(f\"   - Observaciones en train: {selected_series['train_count']}\")\n",
    "    print(f\"   - Observaciones en test: {selected_series['test_count']}\")\n",
    "else:\n",
    "    print(\"\\n‚ùå No se encontraron series disponibles en ambos conjuntos (train y test)\")\n",
    "    selected_series = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09347cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== FILTRAR DATOS DE TRAIN ==========\n",
    "\n",
    "if selected_series is not None:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"FILTRADO DE DATOS DE TRAIN\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Filtrar train_data por los criterios seleccionados\n",
    "    train_filter = (\n",
    "        (train_data['supermarket'] == selected_series['supermarket']) &\n",
    "        (train_data['variant'] == selected_series['variant']) &\n",
    "        (train_data['pack.size'] == selected_series['pack.size'])\n",
    "    )\n",
    "    \n",
    "    train_filtered = train_data[train_filter].copy()\n",
    "    \n",
    "    # Ordenar por fecha si existe la columna date\n",
    "    if 'date' in train_filtered.columns:\n",
    "        train_filtered = train_filtered.sort_values('date').reset_index(drop=True)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Datos de train filtrados:\")\n",
    "    print(f\"   - Observaciones: {len(train_filtered)}\")\n",
    "    print(f\"   - Supermarket: {selected_series['supermarket']}\")\n",
    "    print(f\"   - Variant: {selected_series['variant']}\")\n",
    "    print(f\"   - Pack Size: {selected_series['pack.size']}\")\n",
    "    \n",
    "    # Extraer volume.sales y date\n",
    "    if 'date' in train_filtered.columns:\n",
    "        train_volume = train_filtered['volume.sales'].values\n",
    "        train_dates = train_filtered['date'].values\n",
    "    else:\n",
    "        train_volume = train_filtered['volume.sales'].values\n",
    "        train_dates = train_filtered.index  # Usar √≠ndice si no hay fecha\n",
    "    \n",
    "    print(f\"   - Valores de volume.sales extra√≠dos: {len(train_volume)}\")\n",
    "else:\n",
    "    print(\"‚ùå No hay serie seleccionada. Por favor, ejecuta la celda anterior primero.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f9d5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== FILTRAR DATOS DE TEST ==========\n",
    "\n",
    "if selected_series is not None:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"FILTRADO DE DATOS DE TEST\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Filtrar test_data por los mismos criterios\n",
    "    test_filter = (\n",
    "        (test_data['supermarket'] == selected_series['supermarket']) &\n",
    "        (test_data['variant'] == selected_series['variant']) &\n",
    "        (test_data['pack.size'] == selected_series['pack.size'])\n",
    "    )\n",
    "    \n",
    "    test_filtered = test_data[test_filter].copy()\n",
    "    \n",
    "    # Obtener los √≠ndices originales de test_data que corresponden a las filas filtradas\n",
    "    test_indices = test_data[test_filter].index.tolist()\n",
    "    \n",
    "    # Ordenar por fecha si existe la columna date\n",
    "    if 'date' in test_filtered.columns:\n",
    "        test_filtered = test_filtered.sort_values('date').reset_index(drop=True)\n",
    "        # Reindexar test_indices para mantener el orden\n",
    "        test_indices = test_data[test_filter].sort_values('date').index.tolist()\n",
    "    \n",
    "    print(f\"\\n‚úÖ Datos de test filtrados:\")\n",
    "    print(f\"   - Observaciones: {len(test_filtered)}\")\n",
    "    print(f\"   - Supermarket: {selected_series['supermarket']}\")\n",
    "    print(f\"   - Variant: {selected_series['variant']}\")\n",
    "    print(f\"   - Pack Size: {selected_series['pack.size']}\")\n",
    "    print(f\"   - √çndices originales en test_data: {len(test_indices)}\")\n",
    "    \n",
    "    # Extraer volume.sales y date\n",
    "    if 'date' in test_filtered.columns:\n",
    "        test_volume = test_filtered['volume.sales'].values\n",
    "        test_dates = test_filtered['date'].values\n",
    "    else:\n",
    "        test_volume = test_filtered['volume.sales'].values\n",
    "        test_dates = test_filtered.index  # Usar √≠ndice si no hay fecha\n",
    "    \n",
    "    print(f\"   - Valores de volume.sales extra√≠dos: {len(test_volume)}\")\n",
    "    print(f\"   - Primeros √≠ndices: {test_indices[:5] if len(test_indices) > 5 else test_indices}\")\n",
    "else:\n",
    "    print(\"‚ùå No hay serie seleccionada. Por favor, ejecuta la celda anterior primero.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72f39b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== EXTRAER PREDICCIONES CORRESPONDIENTES ==========\n",
    "\n",
    "if selected_series is not None and 'predictions' in locals():\n",
    "    print(\"=\" * 60)\n",
    "    print(\"EXTRACCI√ìN DE PREDICCIONES CORRESPONDIENTES\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Las predicciones est√°n alineadas con el orden de test_data (√≠ndice por √≠ndice)\n",
    "    # Necesitamos extraer las predicciones que corresponden a los √≠ndices filtrados\n",
    "    \n",
    "    # Convertir predictions a numpy array si es una Series\n",
    "    if isinstance(predictions, pd.Series):\n",
    "        # Si es Series, puede tener √≠ndice alineado con test_data\n",
    "        if hasattr(predictions, 'index') and len(predictions.index) == len(test_data):\n",
    "            # Mapear los √≠ndices originales de test_data a posiciones en predictions\n",
    "            # Crear un diccionario de mapeo: √≠ndice_original -> posici√≥n_en_array\n",
    "            test_data_index_map = {idx: pos for pos, idx in enumerate(test_data.index)}\n",
    "            \n",
    "            # Obtener las posiciones en el array de predictions para cada √≠ndice filtrado\n",
    "            prediction_positions = [test_data_index_map[idx] for idx in test_indices]\n",
    "            predictions_filtered = predictions.iloc[prediction_positions].values\n",
    "        else:\n",
    "            # Si no hay alineaci√≥n clara, asumir que est√° en el mismo orden\n",
    "            predictions_array = predictions.values\n",
    "            # Mapear √≠ndices de test_data a posiciones en el array\n",
    "            test_data_index_map = {idx: pos for pos, idx in enumerate(test_data.index)}\n",
    "            prediction_positions = [test_data_index_map[idx] for idx in test_indices]\n",
    "            predictions_filtered = predictions_array[prediction_positions]\n",
    "    else:\n",
    "        # Si es numpy array, asumir que est√° en el mismo orden que test_data\n",
    "        predictions_array = np.array(predictions)\n",
    "        # Crear mapeo de √≠ndices originales de test_data a posiciones en el array\n",
    "        test_data_index_map = {idx: pos for pos, idx in enumerate(test_data.index)}\n",
    "        # Obtener las posiciones correspondientes a los √≠ndices filtrados\n",
    "        prediction_positions = [test_data_index_map[idx] for idx in test_indices]\n",
    "        predictions_filtered = predictions_array[prediction_positions]\n",
    "    \n",
    "    print(f\"\\n‚úÖ Predicciones extra√≠das:\")\n",
    "    print(f\"   - Total de predicciones en test: {len(predictions) if hasattr(predictions, '__len__') else len(predictions_array)}\")\n",
    "    print(f\"   - Predicciones filtradas para la serie seleccionada: {len(predictions_filtered)}\")\n",
    "    print(f\"   - Media de predicciones filtradas: {predictions_filtered.mean():.2f}\")\n",
    "    print(f\"   - Desviaci√≥n est√°ndar: {predictions_filtered.std():.2f}\")\n",
    "    \n",
    "    # Comparar con valores reales de test\n",
    "    print(f\"\\nüìä Comparaci√≥n:\")\n",
    "    print(f\"   - Media valores reales (test): {test_volume.mean():.2f}\")\n",
    "    print(f\"   - Media predicciones: {predictions_filtered.mean():.2f}\")\n",
    "    print(f\"   - Diferencia media: {abs(test_volume.mean() - predictions_filtered.mean()):.2f}\")\n",
    "    \n",
    "    # Alinear las fechas de test con las predicciones\n",
    "    if 'date' in test_filtered.columns:\n",
    "        prediction_dates = test_dates\n",
    "    else:\n",
    "        prediction_dates = test_dates\n",
    "    \n",
    "    print(f\"   - Fechas correspondientes: {len(prediction_dates)}\")\n",
    "    \n",
    "else:\n",
    "    if selected_series is None:\n",
    "        print(\"‚ùå No hay serie seleccionada. Por favor, ejecuta las celdas anteriores primero.\")\n",
    "    else:\n",
    "        print(\"‚ùå Las predicciones no est√°n disponibles. Por favor, ejecuta la celda de predicciones primero.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34b526b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== VISUALIZACI√ìN DE LA SERIE FILTRADA ==========\n",
    "\n",
    "if selected_series is not None and 'predictions_filtered' in locals():\n",
    "    print(\"=\" * 60)\n",
    "    print(\"VISUALIZACI√ìN DE LA SERIE FILTRADA\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # Crear el gr√°fico\n",
    "    fig, ax = plt.subplots(figsize=(14, 8))\n",
    "    \n",
    "    # Preparar datos para el gr√°fico\n",
    "    # Para train: usar fechas o √≠ndices\n",
    "    if 'date' in train_filtered.columns:\n",
    "        train_x = train_dates\n",
    "    else:\n",
    "        train_x = range(len(train_volume))\n",
    "    \n",
    "    # Para test: continuar desde donde termina train\n",
    "    if 'date' in test_filtered.columns:\n",
    "        test_x = test_dates\n",
    "        prediction_x = prediction_dates\n",
    "    else:\n",
    "        # Si no hay fechas, continuar la numeraci√≥n desde train\n",
    "        test_x = range(len(train_volume), len(train_volume) + len(test_volume))\n",
    "        prediction_x = range(len(train_volume), len(train_volume) + len(predictions_filtered))\n",
    "    \n",
    "    # Plotear datos de train (valores reales)\n",
    "    ax.plot(train_x, train_volume, \n",
    "            label='Train (Valores Reales)', \n",
    "            color='blue', \n",
    "            linewidth=2,\n",
    "            alpha=0.8)\n",
    "    \n",
    "    # Plotear datos de test (valores reales)\n",
    "    ax.plot(test_x, test_volume, \n",
    "            label='Test (Valores Reales)', \n",
    "            color='green', \n",
    "            linewidth=2,\n",
    "            alpha=0.8)\n",
    "    \n",
    "    # Plotear predicciones\n",
    "    ax.plot(prediction_x, predictions_filtered, \n",
    "            label='Predicciones', \n",
    "            color='red', \n",
    "            linestyle='--',\n",
    "            linewidth=2,\n",
    "            alpha=0.8,\n",
    "            marker='o',\n",
    "            markersize=4)\n",
    "    \n",
    "    # Marcar el punto de divisi√≥n entre train y test\n",
    "    if len(train_volume) > 0:\n",
    "        if 'date' in train_filtered.columns:\n",
    "            cutoff_date = train_dates[-1] if hasattr(train_dates, '__getitem__') else train_x[-1]\n",
    "            ax.axvline(x=cutoff_date if hasattr(cutoff_date, '__str__') else train_x[-1], \n",
    "                      color='gray', \n",
    "                      linestyle=':', \n",
    "                      linewidth=2,\n",
    "                      label='Divisi√≥n Train/Test',\n",
    "                      alpha=0.7)\n",
    "        else:\n",
    "            ax.axvline(x=len(train_volume)-0.5, \n",
    "                      color='gray', \n",
    "                      linestyle=':', \n",
    "                      linewidth=2,\n",
    "                      label='Divisi√≥n Train/Test',\n",
    "                      alpha=0.7)\n",
    "    \n",
    "    # Configurar el gr√°fico\n",
    "    title = f\"Serie: {selected_series['supermarket']} | {selected_series['variant']} | {selected_series['pack.size']}\"\n",
    "    ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('Fecha' if 'date' in train_filtered.columns else 'Per√≠odo', fontsize=12)\n",
    "    ax.set_ylabel('Volume Sales', fontsize=12)\n",
    "    ax.legend(loc='best', fontsize=10)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Rotar etiquetas del eje x si hay fechas\n",
    "    if 'date' in train_filtered.columns:\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Mostrar estad√≠sticas resumidas\n",
    "    print(f\"\\nüìä Estad√≠sticas de la serie visualizada:\")\n",
    "    print(f\"   - Train: {len(train_volume)} observaciones\")\n",
    "    print(f\"   - Test: {len(test_volume)} observaciones\")\n",
    "    print(f\"   - Predicciones: {len(predictions_filtered)} observaciones\")\n",
    "    print(f\"\\n   Media Train: {train_volume.mean():.2f}\")\n",
    "    print(f\"   Media Test Real: {test_volume.mean():.2f}\")\n",
    "    print(f\"   Media Predicciones: {predictions_filtered.mean():.2f}\")\n",
    "    \n",
    "    # Calcular m√©tricas b√°sicas si hay suficientes datos\n",
    "    if len(test_volume) == len(predictions_filtered):\n",
    "        mae = np.abs(test_volume - predictions_filtered).mean()\n",
    "        rmse = np.sqrt(((test_volume - predictions_filtered) ** 2).mean())\n",
    "        mape = (np.abs((test_volume - predictions_filtered) / np.maximum(1e-9, np.abs(test_volume))) * 100).mean()\n",
    "        \n",
    "        print(f\"\\n   M√©tricas de predicci√≥n:\")\n",
    "        print(f\"   - MAE: {mae:.2f}\")\n",
    "        print(f\"   - RMSE: {rmse:.2f}\")\n",
    "        print(f\"   - MAPE: {mape:.2f}%\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    \n",
    "else:\n",
    "    if selected_series is None:\n",
    "        print(\"‚ùå No hay serie seleccionada. Por favor, ejecuta las celdas anteriores primero.\")\n",
    "    else:\n",
    "        print(\"‚ùå Las predicciones filtradas no est√°n disponibles. Por favor, ejecuta la celda anterior primero.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TFG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
